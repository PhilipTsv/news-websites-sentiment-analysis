{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsParser = html.fromstring(getRequest(\"https://www.theguardian.com/world/ng-interactive/2020/dec/14/coronavirus-2020-timeline-covid-19\"))\n",
    "#newsParser = html.fromstring(getRequest(\"https://www.theguardian.com/world/2020/oct/26/china-new-coronavirus-outbreak-detected-in-xinjiang-city-of-kashgar\"))\n",
    "#print(newsParser.xpath(\".//h1\"))\n",
    "#for item in newsParser:\n",
    "#      for elt in item.xpath(\".//p\"):\n",
    "#        print(elt.text)\n",
    "\n",
    "#print(len(dfRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#input rows of data\n",
    "#output a pandas dataframe\n",
    "def saveToDf(dfRows):\n",
    "    # create a new dataframe and parser\n",
    "    df = pd.DataFrame()\n",
    "    df = df.append(dfRows, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runProc(country, lock):\n",
    "    from lxml import html, etree\n",
    "    import requests\n",
    "    import urllib3\n",
    "    #import json\n",
    "    #import requests.packages.urllib3.exceptions\n",
    "    from urllib3.exceptions import InsecureRequestWarning\n",
    "    import re\n",
    "    #import datetime\n",
    "    \n",
    "    def getRequest(url):\n",
    "        headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-GB,en;q=0.9,en-US;q=0.8,ml;q=0.7',\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36'\n",
    "        }\n",
    "        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "        response = requests.get(url, verify=False, headers=headers)\n",
    "        return response.text\n",
    "    \n",
    "    def getArticleURLs(countryURL, numOfPages):\n",
    "        urls = []\n",
    "\n",
    "        #scrape the front page\n",
    "        parser = html.fromstring(getRequest(countryURL))\n",
    "        items = parser.xpath(\"//div[@class='fc-item__content ']\")\n",
    "        for item in items:\n",
    "            for elt in item.xpath(\".//h3[@class='fc-item__title']\"):\n",
    "                urls.append(elt[0].attrib['href'])\n",
    "\n",
    "        #scrape the rest of the pages\n",
    "        for i in range(1, numOfPages):\n",
    "            #https://www.theguardian.com/world/china?page=1000\n",
    "            parser = html.fromstring(getRequest(countryURL + \"?page=\" + str(i)))\n",
    "\n",
    "            #<div class=\"fc-item__content \"> \n",
    "            items = parser.xpath(\"//div[@class='fc-item__content ']\")\n",
    "\n",
    "            for item in items:\n",
    "              for elt in item.xpath(\".//h3[@class='fc-item__title']\"):\n",
    "                urls.append(elt[0].attrib['href'])\n",
    "        return urls\n",
    "    \n",
    "    def month_converter(month):\n",
    "        months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "        return months.index(month) + 1\n",
    "\n",
    "    def dateToList(dateList):\n",
    "        dateList[0] = int(dateList[0])\n",
    "        dateList[1] = month_converter(dateList[1])\n",
    "        dateList[2] = int(dateList[2])\n",
    "        return dateList\n",
    "\n",
    "    def isYear(string):\n",
    "        if re.search(\"(?=.*[1][9][0-9][0-9]|[2][0-9][0-9][0-9])\", string) is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #ignoring non-articles with little or no text\n",
    "    def vetUrl(url):\n",
    "        sure = [\"video\",\"gallery\",\"gallery\",\"audio\",\"picture\",\"us-embassy-cables-documents\"]\n",
    "        unsure = [\"blog\",\"from-the-archive-blog\",\"radical-conservation\",\"lostinshowbiz\",\"the-running-blog\",\"australia-books-blog\"]\n",
    "        for s in sure:\n",
    "            if s in url:\n",
    "                return False\n",
    "        for u in unsure:\n",
    "            if u in url:\n",
    "                #printStr = printStr + \"------------------------------\\n\" + url\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def scrapeCategory(urlList):\n",
    "        dfRows = []    \n",
    "        for u in urlList:\n",
    "            url = \"\"\n",
    "            #vet url\n",
    "            if vetUrl(u):\n",
    "                url = u\n",
    "            else:\n",
    "                continue   \n",
    "\n",
    "            newsParser = html.fromstring(getRequest(url))\n",
    "            data = {}\n",
    "            #print(url)\n",
    "            #print title test\n",
    "            #print(newsParser.xpath(\".//h1\")[0].text.strip())\n",
    "\n",
    "            #interactive articles that we don't need\n",
    "            if \"ng-interactive\" in url or \"None\" in url:\n",
    "                continue\n",
    "            #specially formatted Guardian article\n",
    "            elif len(newsParser.xpath(\".//h1\")) == 0:\n",
    "                print(url)\n",
    "                continue\n",
    "            elif newsParser.xpath(\".//h1\")[0].text == None:\n",
    "                for elt in newsParser.xpath(\".//h1\"):\n",
    "                    #print(url)\n",
    "                    data[\"title\"] = elt[0].text.strip()\n",
    "            #normal Guardian article\n",
    "            else:\n",
    "                data[\"title\"] = newsParser.xpath(\".//h1\")[0].text.strip()\n",
    "\n",
    "            #when we get kicked out the h1 (title) changes\n",
    "            if '429 Too Many Requests' in data[\"title\"]:\n",
    "                print(\"Too many requests!\")\n",
    "                print(\"Data gathered: \", len(dfRows))\n",
    "                break\n",
    "\n",
    "            #url is simple\n",
    "            data[\"url\"] = url\n",
    "\n",
    "            #date\n",
    "            #get all elements of the url and loop through them\n",
    "            elements = u.rstrip().split('/')\n",
    "            for i in range(0, len(elements)):\n",
    "                #if a i == year, it means a date follows [i:i+3]\n",
    "                if elements[i].isnumeric() and isYear(elements[i]):\n",
    "                    try:\n",
    "                        #date always starts from i\n",
    "                        data[\"date\"] = dateToList(elements[i:i+3])\n",
    "                    #in case we give the date function a bad string\n",
    "                    except IndexError:\n",
    "                        print(url)\n",
    "                    #in case we find a new sub-category \n",
    "                    except ValueError:\n",
    "                        print(elements[i])\n",
    "\n",
    "            #get article text - for Guardian - always stored in <p>\n",
    "            #collect all paragraphs\n",
    "            paragraphList = []\n",
    "            for item in newsParser:\n",
    "                for elt in item.xpath(\".//p\"):\n",
    "                    paragraphList.append(elt.text)\n",
    "\n",
    "            #first one is always the subtitle\n",
    "            data[\"subtitle\"] = paragraphList.pop(0)\n",
    "\n",
    "            #collect raw article text\n",
    "            data[\"text\"] = ''\n",
    "            for paragraph in paragraphList:\n",
    "                #print(paragraph)\n",
    "                #if it's None for some reason: continue\n",
    "                if isinstance(paragraph, str) is False:\n",
    "                    continue\n",
    "                #Don't need this: we have published date\n",
    "                elif \"Last modified on \" in paragraph:\n",
    "                    continue\n",
    "                #some times <p> are something weird like above;\n",
    "                try:\n",
    "                    data[\"text\"] = data[\"text\"] + paragraph.strip()\n",
    "                #so if it is: split it out and continue\n",
    "                except TypeError:\n",
    "                    print(\"Type Error!\")\n",
    "                    print(paragraph)\n",
    "                    continue\n",
    "\n",
    "            #write data entry to list\n",
    "            dfRows.append(data)\n",
    "            if(len(dfRows) % 5000):\n",
    "                print(str(len(dfRows)), url)\n",
    "            #continue after a delay, so we don't go over request limit; can't be lower than this\n",
    "            time.sleep(900/1000)\n",
    "\n",
    "        #when done\n",
    "        print(str(len(dfRows)) + \" entries!\")\n",
    "        print(\"Scraping finished!\")\n",
    "\n",
    "    urls = getArticleURLs(country[1], country[2])\n",
    "    dataRows = scrapeCategory(urls)\n",
    "    df = saveToDf(dataRows)\n",
    "    \n",
    "    lock.acquire()\n",
    "    dfs.append(df)\n",
    "    lock.release()\n",
    "    df.to_csv(country[1] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping processes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-4823f763f541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting scraping processes...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproc_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-4823f763f541>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting scraping processes...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproc_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\multiprocess\\process.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can only join a child process'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can only join a started process'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\multiprocess\\popen_spawn_win32.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[0mmsecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForSingleObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWAIT_OBJECT_0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetExitCodeProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocess\n",
    "import time\n",
    "\n",
    "countryList = [('China', 'https://www.theguardian.com/world/china', 1101),\n",
    "                ('USA', 'https://www.theguardian.com/us-news', 1900),\n",
    "                ('Germany', 'https://www.theguardian.com/world/germany', 622),\n",
    "                ('Russia', 'https://www.theguardian.com/world/russia', 970),\n",
    "                ('Japan', 'https://www.theguardian.com/world/japan', 370),\n",
    "                ('North Korea', 'https://www.theguardian.com/world/north-korea', 244),\n",
    "                ('South Korea', 'https://www.theguardian.com/world/south-korea', 143),\n",
    "                ('Australia', 'https://www.theguardian.com/australia-news', 3566),\n",
    "                ('India', 'https://www.theguardian.com/world/india', 544)]\n",
    "dfs = []\n",
    "threadLock = multiprocess.Lock()\n",
    "proc_list = []\n",
    "\n",
    "for values in countryList:\n",
    "    #print([values[0], values[1], threadLock])\n",
    "    p = multiprocess.Process(target=runProc, args=[values, threadLock])\n",
    "    proc_list.append(p)\n",
    "    p.start()  # start execution of printer\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print('Starting scraping processes...')\n",
    "\n",
    "[p.join() for p in proc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df['tiltle']==None))\n",
    "#print(len(df['url']==None))\n",
    "#print(len(df['subtitle']==None))\n",
    "#print(len(df['text']==None))\n",
    "#print(df[df['url']=='https://www.theguardian.com/football/2019/dec/13/arsenal-distance-themselves-from-mesut-ozil-comments-china-uighur-people'])\n",
    "#print(df[df['title']=='429 Too Many Requests'])\n",
    "#temp = \"https://www.theguardian.com/books/australia-books-blog/2019/apr/29/les-murray-remembered-by-john-kinsella-a-brilliant-battler-poet\".split('/')\n",
    "#temp = \"https://www.theguardian.com/artanddesign/gallery/2015/feb/13/red-army-primary-school-gallery\".split('/')#[5:8]\n",
    "#print(temp)\n",
    "for dataframe in dfs:\n",
    "    fataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data.1ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
