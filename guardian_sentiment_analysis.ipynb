{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D4y_QcqVmf8n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "suPrNJjgmf8w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_converter(month):\n",
    "    months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "    return months.index(month) + 1\n",
    "\n",
    "def dateToList(dateList):\n",
    "    dateList[0] = int(dateList[0])\n",
    "    dateList[1] = month_converter(dateList[1])\n",
    "    dateList[2] = int(dateList[2])\n",
    "    return dateList\n",
    "\n",
    "def isYear(string):\n",
    "    if re.search(\"(?=.*[1][9][0-9][0-9]|[2][0-9][0-9][0-9])\", string) is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "#ignoring non-articles with little or no text\n",
    "def vetUrl(url):\n",
    "    sure = [\"video\",\"gallery\",\"gallery\",\"audio\",\"picture\",\"us-embassy-cables-documents\"]\n",
    "    unsure = [\"blog\",\"from-the-archive-blog\",\"radical-conservation\",\"lostinshowbiz\",\"the-running-blog\",\"australia-books-blog\"]\n",
    "    for s in sure:\n",
    "        if s in url:\n",
    "            return False\n",
    "    for u in unsure:\n",
    "        if u in url:\n",
    "            #printStr = printStr + \"------------------------------\\n\" + url\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrapeCategory(urlList):\n",
    "    dfRows = []    \n",
    "    for u in urlList:\n",
    "        url = \"\"\n",
    "        #vet url\n",
    "        if vetUrl(u):\n",
    "            url = u\n",
    "        else:\n",
    "            continue   \n",
    "                \n",
    "        newsParser = html.fromstring(getRequest(url))\n",
    "        data = {}\n",
    "        #print(url)\n",
    "        #print title test\n",
    "        #print(newsParser.xpath(\".//h1\")[0].text.strip())\n",
    "\n",
    "        #interactive articles that we don't need\n",
    "        if \"ng-interactive\" in url or \"None\" in url:\n",
    "            continue\n",
    "        #specially formatted Guardian article\n",
    "        elif len(newsParser.xpath(\".//h1\")) == 0:\n",
    "            print(url)\n",
    "            continue\n",
    "        elif newsParser.xpath(\".//h1\")[0].text == None:\n",
    "            for elt in newsParser.xpath(\".//h1\"):\n",
    "                #print(url)\n",
    "                data[\"title\"] = elt[0].text.strip()\n",
    "        #normal Guardian article\n",
    "        else:\n",
    "            data[\"title\"] = newsParser.xpath(\".//h1\")[0].text.strip()\n",
    "\n",
    "        #when we get kicked out the h1 (title) changes\n",
    "        if '429 Too Many Requests' in data[\"title\"]:\n",
    "            print(\"Too many requests!\")\n",
    "            print(\"Data gathered: \", len(dfRows))\n",
    "            break\n",
    "\n",
    "        #url is simple\n",
    "        data[\"url\"] = url\n",
    "\n",
    "        #date\n",
    "        #get all elements of the url and loop through them\n",
    "        elements = u.rstrip().split('/')\n",
    "        for i in range(0, len(elements)):\n",
    "            #if a i == year, it means a date follows [i:i+3]\n",
    "            if elements[i].isnumeric() and isYear(elements[i]):\n",
    "                try:\n",
    "                    #date always starts from i\n",
    "                    data[\"date\"] = dateToList(elements[i:i+3])\n",
    "                #in case we give the date function a bad string\n",
    "                except IndexError:\n",
    "                    print(url)\n",
    "                #in case we find a new sub-category \n",
    "                except ValueError:\n",
    "                    print(elements[i])\n",
    "\n",
    "        #get article text - for Guardian - always stored in <p>\n",
    "        #collect all paragraphs\n",
    "        paragraphList = []\n",
    "        for item in newsParser:\n",
    "            for elt in item.xpath(\".//p\"):\n",
    "                paragraphList.append(elt.text)\n",
    "\n",
    "        #first one is always the subtitle\n",
    "        data[\"subtitle\"] = paragraphList.pop(0)\n",
    "\n",
    "        #collect raw article text\n",
    "        data[\"text\"] = ''\n",
    "        for paragraph in paragraphList:\n",
    "            #print(paragraph)\n",
    "            #if it's None for some reason: continue\n",
    "            if isinstance(paragraph, str) is False:\n",
    "                continue\n",
    "            #Don't need this: we have published date\n",
    "            elif \"Last modified on \" in paragraph:\n",
    "                continue\n",
    "            #some times <p> are something weird like above;\n",
    "            try:\n",
    "                data[\"text\"] = data[\"text\"] + paragraph.strip()\n",
    "            #so if it is: split it out and continue\n",
    "            except TypeError:\n",
    "                print(\"Type Error!\")\n",
    "                print(paragraph)\n",
    "                continue\n",
    "\n",
    "        #write data entry to list\n",
    "        dfRows.append(data)\n",
    "        #continue after a delay, so we don't go over request limit; can't be lower than this\n",
    "        time.sleep(900/1000)\n",
    "\n",
    "    #when done\n",
    "    print(str(len(dfRows)) + \" entries!\")\n",
    "    print(\"Scraping finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsParser = html.fromstring(getRequest(\"https://www.theguardian.com/world/ng-interactive/2020/dec/14/coronavirus-2020-timeline-covid-19\"))\n",
    "#newsParser = html.fromstring(getRequest(\"https://www.theguardian.com/world/2020/oct/26/china-new-coronavirus-outbreak-detected-in-xinjiang-city-of-kashgar\"))\n",
    "#print(newsParser.xpath(\".//h1\"))\n",
    "#for item in newsParser:\n",
    "#      for elt in item.xpath(\".//p\"):\n",
    "#        print(elt.text)\n",
    "\n",
    "#print(len(dfRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#input rows of data\n",
    "#output a pandas dataframe\n",
    "def saveToDf(dfRows):\n",
    "    # create a new dataframe and parser\n",
    "    df = pd.DataFrame()\n",
    "    df = df.append(dfRows, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runProc(country, lock):\n",
    "    from lxml import html, etree\n",
    "    import requests\n",
    "    import urllib3\n",
    "    #import json\n",
    "    #import requests.packages.urllib3.exceptions\n",
    "    from urllib3.exceptions import InsecureRequestWarning\n",
    "    #import re\n",
    "    #import datetime\n",
    "    \n",
    "    def getRequest(url):\n",
    "        headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-GB,en;q=0.9,en-US;q=0.8,ml;q=0.7',\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36'\n",
    "        }\n",
    "        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "        response = requests.get(url, verify=False, headers=headers)\n",
    "        return response.text\n",
    "    \n",
    "    def getArticleURLs(countryURL, numOfPages):\n",
    "        urls = []\n",
    "\n",
    "        #scrape the front page\n",
    "        parser = html.fromstring(getRequest(countryURL))\n",
    "        items = parser.xpath(\"//div[@class='fc-item__content ']\")\n",
    "        for item in items:\n",
    "            for elt in item.xpath(\".//h3[@class='fc-item__title']\"):\n",
    "                urls.append(elt[0].attrib['href'])\n",
    "\n",
    "        #scrape the rest of the pages\n",
    "        for i in range(1, numOfPages):\n",
    "            #https://www.theguardian.com/world/china?page=1000\n",
    "            parser = html.fromstring(getRequest(countryURL + \"?page=\" + str(i)))\n",
    "\n",
    "            #<div class=\"fc-item__content \"> \n",
    "            items = parser.xpath(\"//div[@class='fc-item__content ']\")\n",
    "\n",
    "            for item in items:\n",
    "              for elt in item.xpath(\".//h3[@class='fc-item__title']\"):\n",
    "                urls.append(elt[0].attrib['href'])\n",
    "\n",
    "        return urls\n",
    "\n",
    "    urls = getArticleURLs(country[1], country[2])\n",
    "    dataRows = scrapeCategory(urls)\n",
    "    df = saveToDf(dataRows)\n",
    "    \n",
    "    lock.acquire()\n",
    "    dfs.append(df)\n",
    "    lock.release()\n",
    "    #df.to_csv(country[1] + '.csv')\n",
    "\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not waiting for proccesses to finish...\n"
     ]
    }
   ],
   "source": [
    "import multiprocess\n",
    "import time\n",
    "\n",
    "countryList = [('China', 'https://www.theguardian.com/world/china', 1101),\n",
    "                    ('USA', 'https://www.theguardian.com/us-news', 1900),\n",
    "                    ('Germany', 'https://www.theguardian.com/world/germany', 622),\n",
    "                    ('Russia', 'https://www.theguardian.com/world/russia', 970),\n",
    "                    ('Japan', 'https://www.theguardian.com/world/japan', 370),\n",
    "                   ('North Korea', 'https://www.theguardian.com/world/north-korea', 244),\n",
    "                   ('South Korea', 'https://www.theguardian.com/world/south-korea', 143),\n",
    "                   ('Australia', 'https://www.theguardian.com/australia-news', 3566),\n",
    "                   ('India', 'https://www.theguardian.com/world/india', 544)]\n",
    "dfs = []\n",
    "threadLock = multiprocess.Lock()\n",
    "proc_list = []\n",
    "\n",
    "for values in countryList:\n",
    "    #print([values[0], values[1], threadLock])\n",
    "    p = multiprocess.Process(target=runProc, args=[values, threadLock])\n",
    "    proc_list.append(p)\n",
    "    p.start()  # start execution of printer\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print('Now waiting for proccesses to finish...')\n",
    "\n",
    "[p.join() for p in proc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df['tiltle']==None))\n",
    "#print(len(df['url']==None))\n",
    "#print(len(df['subtitle']==None))\n",
    "#print(len(df['text']==None))\n",
    "#print(df[df['url']=='https://www.theguardian.com/football/2019/dec/13/arsenal-distance-themselves-from-mesut-ozil-comments-china-uighur-people'])\n",
    "#print(df[df['title']=='429 Too Many Requests'])\n",
    "#temp = \"https://www.theguardian.com/books/australia-books-blog/2019/apr/29/les-murray-remembered-by-john-kinsella-a-brilliant-battler-poet\".split('/')\n",
    "#temp = \"https://www.theguardian.com/artanddesign/gallery/2015/feb/13/red-army-primary-school-gallery\".split('/')#[5:8]\n",
    "#print(temp)\n",
    "dfUsa = dfs[0]\n",
    "dfUsa.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data.1ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
